# åŒ»é™¢å®¢æˆ·ä¿¡æ¯è‡ªåŠ¨é‡‡é›†ç³»ç»Ÿ - å¿«é€Ÿå¯åŠ¨æŒ‡å—

## ğŸ“‹ ç³»ç»Ÿç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–é‡‡é›†åŒ»é™¢ç³»ç»Ÿè½¯ç¡¬ä»¶å»ºè®¾ä¿¡æ¯çš„å·¥å…·ï¼Œä¸»è¦æ•°æ®æ¥æºåŒ…æ‹¬ï¼š
- ç¦å»ºçœæ”¿åºœé‡‡è´­ç½‘
- åŒ»é™¢å®˜ç½‘å’Œå…¬ä¼—å·
- ä¾›åº”å•†å…¬ä¼—å·
- å…¶ä»–å…¬å¼€æ¸ é“

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
1. âœ… è‡ªåŠ¨é‡‡é›†æ”¿åºœé‡‡è´­å…¬å‘Š
2. âœ… æå–ç³»ç»Ÿå»ºè®¾ä¿¡æ¯
3. âœ… æ•´åˆåˆ°å®¢æˆ·æ¡£æ¡ˆExcel
4. âœ… AIè¾…åŠ©ä¿¡æ¯ç»“æ„åŒ–ï¼ˆå¯é€‰ï¼‰
5. âœ… å®šæ—¶è‡ªåŠ¨æ›´æ–°

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿä¸Šæ‰‹ï¼‰

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

```bash
# 1. ç¡®ä¿å®‰è£…äº† Python 3.8+
python --version

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv

# Windowsæ¿€æ´»
venv\Scripts\activate

# Mac/Linuxæ¿€æ´»
source venv/bin/activate

# 3. å®‰è£…ä¾èµ–ï¼ˆæ ¸å¿ƒåŒ…ï¼‰
pip install pandas openpyxl requests beautifulsoup4 lxml
```

### ç¬¬äºŒæ­¥ï¼šè¿è¡Œé‡‡é›†ç¨‹åº

```bash
# è¿è¡Œæ”¿åºœé‡‡è´­ç½‘çˆ¬è™«
python gov_procurement_crawler.py
```

ç¨‹åºä¼šè‡ªåŠ¨ï¼š
1. ä»ä½ çš„å®¢æˆ·æ¡£æ¡ˆExcelè¯»å–åŒ»é™¢åˆ—è¡¨
2. æœç´¢æ¯å®¶åŒ»é™¢çš„é‡‡è´­ä¿¡æ¯ï¼ˆè¿‘1å¹´ï¼‰
3. ä¿å­˜ç»“æœåˆ° `æ”¿åºœé‡‡è´­æ•°æ®_æ—¥æœŸ.xlsx`

**é¢„è®¡è€—æ—¶**: 10-30åˆ†é’Ÿï¼ˆå–å†³äºåŒ»é™¢æ•°é‡ï¼‰

### ç¬¬ä¸‰æ­¥ï¼šæ•°æ®æ•´åˆ

```bash
# å°†é‡‡é›†çš„æ•°æ®æ•´åˆåˆ°å®¢æˆ·æ¡£æ¡ˆ
python data_integrator.py
```

ç¨‹åºä¼šè‡ªåŠ¨ï¼š
1. è¯»å–é‡‡é›†çš„æ”¿åºœé‡‡è´­æ•°æ®
2. æ›´æ–°å®¢æˆ·æ¡£æ¡ˆä¸­çš„"è½¯ç¡¬ä»¶å»ºè®¾æƒ…å†µ"å­—æ®µ
3. ç”Ÿæˆæ›´æ–°æŠ¥å‘Š
4. ä¿å­˜ä¸ºæ–°çš„Excelæ–‡ä»¶ï¼ˆåŸæ–‡ä»¶ä¸ä¼šè¢«è¦†ç›–ï¼‰

---

## ğŸ“ é¡¹ç›®æ–‡ä»¶è¯´æ˜

```
åŒ»é™¢å®¢æˆ·ä¿¡æ¯è‡ªåŠ¨é‡‡é›†ç³»ç»Ÿ/
â”‚
â”œâ”€â”€ gov_procurement_crawler.py      # æ ¸å¿ƒçˆ¬è™«ç¨‹åº
â”œâ”€â”€ data_integrator.py             # æ•°æ®æ•´åˆç¨‹åº
â”œâ”€â”€ requirements.txt               # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ åŒ»é™¢å®¢æˆ·ä¿¡æ¯è‡ªåŠ¨åŒ–æ”¶é›†æ–¹æ¡ˆ.md  # å®Œæ•´æ–¹æ¡ˆæ–‡æ¡£
â””â”€â”€ README.md                      # æœ¬æ–‡ä»¶
```

---

## ğŸ”§ è¯¦ç»†é…ç½®

### é…ç½®1ï¼šä¿®æ”¹æœç´¢æ—¶é—´èŒƒå›´

ç¼–è¾‘ `gov_procurement_crawler.py`ï¼Œæ‰¾åˆ° `main()` å‡½æ•°ï¼š

```python
# é»˜è®¤æœç´¢æœ€è¿‘365å¤©
crawler.search_hospital_procurement(hospital, days_back=365)

# æ”¹ä¸ºæœç´¢æœ€è¿‘2å¹´
crawler.search_hospital_procurement(hospital, days_back=730)
```

### é…ç½®2ï¼šè°ƒæ•´ç³»ç»Ÿå…³é”®è¯

åœ¨ `gov_procurement_crawler.py` ä¸­æ‰¾åˆ° `system_keywords`ï¼š

```python
self.system_keywords = {
    'OAç³»ç»Ÿ': ['OA', 'åŠå…¬è‡ªåŠ¨åŒ–', 'ååŒåŠå…¬'],
    'HISç³»ç»Ÿ': ['HIS', 'åŒ»é™¢ä¿¡æ¯ç³»ç»Ÿ'],
    # æ·»åŠ æ›´å¤šå…³é”®è¯
    'æ£€éªŒç³»ç»Ÿ': ['LIS', 'æ£€éªŒä¿¡æ¯ç³»ç»Ÿ'],
}
```

### é…ç½®3ï¼šå¯ç”¨AIå¢å¼ºåŠŸèƒ½ï¼ˆå¯é€‰ï¼‰

```bash
# 1. å®‰è£…AIåº“
pip install anthropic

# 2. è®¾ç½®API Key
export ANTHROPIC_API_KEY='your-key-here'  # Mac/Linux
set ANTHROPIC_API_KEY=your-key-here       # Windows

# 3. åœ¨ä»£ç ä¸­å¯ç”¨AI
# ç¼–è¾‘ gov_procurement_crawler.pyï¼Œåœ¨ main() ä¸­è®¾ç½® use_ai = True
```

---

## ğŸ“Š å®é™…ç½‘ç«™é€‚é…æŒ‡å—

âš ï¸ **é‡è¦æç¤º**: æä¾›çš„ä»£ç æ˜¯**ç¤ºä¾‹æ¡†æ¶**ï¼Œéœ€è¦æ ¹æ®å®é™…ç½‘ç«™çš„HTMLç»“æ„è¿›è¡Œè°ƒæ•´ã€‚

### é€‚é…æ­¥éª¤

#### 1. åˆ†æç›®æ ‡ç½‘ç«™ç»“æ„

```bash
# è®¿é—®ç¦å»ºçœæ”¿åºœé‡‡è´­ç½‘
# è§‚å¯Ÿæœç´¢ç»“æœé¡µé¢çš„HTMLç»“æ„
# ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ï¼ˆF12ï¼‰æŸ¥çœ‹å…ƒç´ 
```

#### 2. ä¿®æ”¹è§£æä»£ç 

åœ¨ `gov_procurement_crawler.py` ä¸­æ‰¾åˆ° `_parse_procurement_item()` å‡½æ•°ï¼š

```python
def _parse_procurement_item(self, item):
    # æ ¹æ®å®é™…HTMLä¿®æ”¹ä»¥ä¸‹é€‰æ‹©å™¨
    title = item.find('a', class_='å®é™…çš„classåç§°').text.strip()
    link = item.find('a', class_='å®é™…çš„classåç§°')['href']
    pub_date = item.find('span', class_='å®é™…çš„classåç§°').text.strip()
    # ...
```

#### 3. æµ‹è¯•å•ä¸ªåŒ»é™¢

```python
# åœ¨ main() ä¸­åªæµ‹è¯•ä¸€å®¶åŒ»é™¢
hospitals = ['ç¦å·å¤§å­¦é™„å±çœç«‹åŒ»é™¢']
crawler.search_hospital_procurement(hospitals[0], days_back=30)
```

#### 4. é€æ­¥æ‰©å±•

ç¡®è®¤å•ä¸ªåŒ»é™¢é‡‡é›†æˆåŠŸåï¼Œå†æ‰©å±•åˆ°æ›´å¤šåŒ»é™¢ã€‚

---

## ğŸŒ æ”¯æŒçš„æ•°æ®æº

### å·²å®ç°
- âœ… ç¦å»ºçœæ”¿åºœé‡‡è´­ç½‘ï¼ˆéœ€é€‚é…ï¼‰

### å¾…å®ç°ï¼ˆå‚è€ƒå®Œæ•´æ–¹æ¡ˆæ–‡æ¡£ï¼‰
- â­ ä¸­å›½æ”¿åºœé‡‡è´­ç½‘
- â­ å„åœ°å¸‚æ”¿åºœé‡‡è´­ç½‘
- â­ åŒ»é™¢å®˜ç½‘
- â­ å¾®ä¿¡å…¬ä¼—å·ï¼ˆé€šè¿‡æœç‹—å¾®ä¿¡æœç´¢ï¼‰

---

## ğŸ¤– AIåŠŸèƒ½è¯´æ˜

### åŸºç¡€ç‰ˆï¼ˆæ— éœ€AIï¼‰
- âœ… è‡ªåŠ¨é‡‡é›†é‡‡è´­å…¬å‘Š
- âœ… æå–åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€é‡‘é¢ã€ä¾›åº”å•†ï¼‰
- âœ… æ•´åˆåˆ°Excel

### AIå¢å¼ºç‰ˆï¼ˆéœ€è¦API Keyï¼‰
- âœ… æ™ºèƒ½ç†è§£é‡‡è´­å†…å®¹
- âœ… è‡ªåŠ¨æå–è½¯ç¡¬ä»¶é…ç½®
- âœ… è¯†åˆ«æŠ€æœ¯æ¶æ„
- âœ… ç»“æ„åŒ–è¾“å‡º

**æˆæœ¬é¢„ä¼°**: æ¯æ¡é‡‡è´­å…¬å‘Šçº¦ 0.01-0.03 å…ƒï¼ˆClaude APIï¼‰

---

## ğŸ“… å®šæ—¶ä»»åŠ¡è®¾ç½®

### æ–¹å¼1ï¼šä½¿ç”¨ schedule åº“ï¼ˆæ¨èåˆå­¦è€…ï¼‰

åœ¨ `gov_procurement_crawler.py` æœ€åæ·»åŠ ï¼š

```python
import schedule
import time

def daily_job():
    print(f"å¼€å§‹å®šæ—¶ä»»åŠ¡: {datetime.now()}")
    main()

# æ¯å¤©å‡Œæ™¨2ç‚¹è¿è¡Œ
schedule.every().day.at("02:00").do(daily_job)

# æˆ–è€…æ¯å‘¨ä¸€è¿è¡Œ
# schedule.every().monday.at("02:00").do(daily_job)

print("å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯å¤©2ç‚¹è‡ªåŠ¨è¿è¡Œ...")
while True:
    schedule.run_pending()
    time.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
```

### æ–¹å¼2ï¼šä½¿ç”¨ç³»ç»Ÿå®šæ—¶ä»»åŠ¡

**Linux/Mac (cron)**:
```bash
# ç¼–è¾‘ crontab
crontab -e

# æ·»åŠ ä»»åŠ¡ï¼ˆæ¯å¤©2ç‚¹è¿è¡Œï¼‰
0 2 * * * cd /path/to/project && /path/to/python gov_procurement_crawler.py >> crawler.log 2>&1
```

**Windows (ä»»åŠ¡è®¡åˆ’ç¨‹åº)**:
1. æ‰“å¼€"ä»»åŠ¡è®¡åˆ’ç¨‹åº"
2. åˆ›å»ºåŸºæœ¬ä»»åŠ¡
3. è®¾ç½®è§¦å‘å™¨ï¼ˆæ¯å¤©2ç‚¹ï¼‰
4. æ“ä½œï¼šå¯åŠ¨ç¨‹åº `python.exe`
5. å‚æ•°ï¼š`gov_procurement_crawler.py`

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è¿è¡ŒæŠ¥é”™ "No module named 'xxx'"
```bash
# å®‰è£…ç¼ºå¤±çš„åº“
pip install xxx
```

### Q2: é‡‡é›†ä¸åˆ°æ•°æ®
1. æ£€æŸ¥ç½‘ç«™æ˜¯å¦å¯è®¿é—®
2. æŸ¥çœ‹ç½‘ç«™HTMLç»“æ„æ˜¯å¦å˜åŒ–
3. è°ƒæ•´ `_parse_procurement_item()` ä¸­çš„é€‰æ‹©å™¨

### Q3: Excelæ‰“ä¸å¼€æˆ–æ ¼å¼é”™è¯¯
```bash
# é‡æ–°å®‰è£… openpyxl
pip uninstall openpyxl
pip install openpyxl==3.1.2
```

### Q4: è¯·æ±‚è¢«é˜»æ­¢ï¼ˆ403/429é”™è¯¯ï¼‰
```python
# å¢åŠ è¯·æ±‚é—´éš”
time.sleep(5)  # ä»2ç§’æ”¹ä¸º5ç§’

# ä½¿ç”¨ä»£ç†IPï¼ˆéœ€é¢å¤–é…ç½®ï¼‰
proxies = {'http': 'http://proxy:port'}
requests.get(url, proxies=proxies)
```

### Q5: AIæå–å¤±è´¥
1. æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥ç½‘ç»œè¿æ¥
3. æŸ¥çœ‹APIä½¿ç”¨é¢åº¦

---

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### æŠ€å·§1ï¼šåˆ†æ‰¹é‡‡é›†
å¦‚æœåŒ»é™¢æ•°é‡å¾ˆå¤šï¼Œå»ºè®®åˆ†æ‰¹é‡‡é›†ï¼š

```python
# æ¯æ¬¡é‡‡é›†10å®¶
for i in range(0, len(hospitals), 10):
    batch = hospitals[i:i+10]
    for hospital in batch:
        crawler.search_hospital_procurement(hospital)
    # æ‰¹æ¬¡é—´ä¼‘æ¯10åˆ†é’Ÿ
    time.sleep(600)
```

### æŠ€å·§2ï¼šå¢é‡æ›´æ–°
åªé‡‡é›†æœ€è¿‘çš„æ•°æ®ï¼š

```python
# åªæœç´¢æœ€è¿‘7å¤©
crawler.search_hospital_procurement(hospital, days_back=7)
```

### æŠ€å·§3ï¼šæ•°æ®å»é‡
åœ¨ `data_integrator.py` ä¸­æ·»åŠ å»é‡é€»è¾‘ï¼š

```python
def remove_duplicates(data):
    seen = set()
    unique_data = []
    for item in data:
        key = (item['title'], item['pub_date'])
        if key not in seen:
            seen.add(key)
            unique_data.append(item)
    return unique_data
```

### æŠ€å·§4ï¼šç›‘æ§æ—¥å¿—
æ·»åŠ è¯¦ç»†æ—¥å¿—è®°å½•ï¼š

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### ä¼˜åŒ–1ï¼šå¹¶å‘é‡‡é›†
ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿï¼š

```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=3) as executor:
    executor.map(crawler.search_hospital_procurement, hospitals)
```

### ä¼˜åŒ–2ï¼šç¼“å­˜æœºåˆ¶
é¿å…é‡å¤é‡‡é›†ï¼š

```python
import hashlib
import json

def cache_result(url, data):
    cache_key = hashlib.md5(url.encode()).hexdigest()
    with open(f'cache/{cache_key}.json', 'w') as f:
        json.dump(data, f)
```

### ä¼˜åŒ–3ï¼šæ•°æ®åº“å­˜å‚¨
å¯¹äºå¤§é‡æ•°æ®ï¼Œä½¿ç”¨æ•°æ®åº“ï¼š

```python
import sqlite3

conn = sqlite3.connect('procurement.db')
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS procurement (
        id INTEGER PRIMARY KEY,
        hospital TEXT,
        title TEXT,
        amount TEXT,
        pub_date TEXT
    )
''')
```

---

## ğŸ”’ å®‰å…¨ä¸åˆè§„

### éµå®ˆ robots.txt
```bash
# æ£€æŸ¥ç½‘ç«™æ˜¯å¦å…è®¸çˆ¬å–
curl http://ccgp-fujian.gov.cn/robots.txt
```

### æ§åˆ¶è¯·æ±‚é¢‘ç‡
```python
# å»ºè®®æ¯ä¸ªè¯·æ±‚é—´éš”3-5ç§’
time.sleep(3)
```

### è®¾ç½®è¶…æ—¶
```python
response = requests.get(url, timeout=10)
```

### ä½¿ç”¨User-Agent
```python
headers = {
    'User-Agent': 'Mozilla/5.0 ...'
}
```

---

## ğŸ“§ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æŸ¥çœ‹å®Œæ•´æ–¹æ¡ˆæ–‡æ¡£ï¼š`åŒ»é™¢å®¢æˆ·ä¿¡æ¯è‡ªåŠ¨åŒ–æ”¶é›†æ–¹æ¡ˆ.md`
2. æ£€æŸ¥é”™è¯¯æ—¥å¿—ï¼š`crawler.log`
3. æµ‹è¯•å•ä¸ªåŒ»é™¢æ˜¯å¦æ­£å¸¸å·¥ä½œ
4. éªŒè¯ç½‘ç«™æ˜¯å¦å¯è®¿é—®

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0 (2025-10-19)
- âœ… åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… å®ç°æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«æ¡†æ¶
- âœ… å®ç°æ•°æ®æ•´åˆåŠŸèƒ½
- âœ… æ”¯æŒAIå¢å¼ºæå–ï¼ˆå¯é€‰ï¼‰

---

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

1. æ·»åŠ æ›´å¤šæ•°æ®æºï¼ˆåŒ»é™¢å®˜ç½‘ã€å…¬ä¼—å·ï¼‰
2. å®ç°Webå¯è§†åŒ–ç•Œé¢
3. å¢åŠ æ•°æ®éªŒè¯å’Œå»é‡
4. ä¼˜åŒ–AIæå–å‡†ç¡®åº¦
5. æ·»åŠ ç§»åŠ¨ç«¯æé†’åŠŸèƒ½

---

## âš–ï¸ å…è´£å£°æ˜

æœ¬å·¥å…·ä»…ä¾›å­¦ä¹ å’Œå†…éƒ¨ä½¿ç”¨ã€‚ä½¿ç”¨æ—¶è¯·ï¼š
- éµå®ˆç½‘ç«™æœåŠ¡æ¡æ¬¾
- æ§åˆ¶è®¿é—®é¢‘ç‡
- ä¸è¦ç”¨äºå•†ä¸šç›®çš„
- æ•°æ®ä»…ä¾›å‚è€ƒ

---

**ç¥ä½¿ç”¨æ„‰å¿«ï¼æœ‰é—®é¢˜æ¬¢è¿åé¦ˆã€‚**
# crm_spyder
