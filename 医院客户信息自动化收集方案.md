# 医院客户信息自动化收集方案

## 一、数据源梳理与扩展

### 1.1 已知数据源
- ✅ 福建省政府采购网
- ✅ 卖方公司公众号
- ✅ 买方医院官网和公众号

### 1.2 建议新增数据源（重要！）
- 🔥 **中国政府采购网** (http://www.ccgp.gov.cn)
- 🔥 **各地市政府采购网** (如：福州市、厦门市、泉州市等)
- 🔥 **医院招标公告栏** (医院官网的招标采购板块)
- 🔥 **医疗卫生信息化行业媒体**
  - HC3i数字医疗网
  - 健康界
  - 中国数字医疗网
- 🔥 **企业工商信息网站** (天眼查/企查查 - 查询中标公司背景)
- 🔥 **医院信息化案例库**
  - CHIMA官网(中国医院协会信息专业委员会)
  - 医疗信息化厂商官网案例
- 🔥 **行业展会新闻** (CHINC大会、CHIMA大会报道)

---

## 二、自动化方案设计

### 方案架构
```
数据采集层 → 数据清洗层 → 数据匹配层 → 数据存储层 → 数据更新层
```

### 2.1 核心爬虫模块

#### A. 政府采购网爬虫 (最重要！)

**目标数据**：
- 项目名称
- 采购单位（医院名称）
- 中标单位（供应商）
- 中标金额
- 项目内容（系统名称、软硬件配置）
- 中标时间
- 项目周期

**技术方案**：
```python
# 核心爬虫框架（基于 Scrapy + Selenium）
网站类型：
1. 福建省政府采购网: http://ccgp-fujian.gov.cn/
2. 中国政府采购网福建站: http://www.ccgp.gov.cn/cggg/dfgg/fjzfcgw/
3. 各地市采购网（福州、厦门等）

采集策略：
- 关键词搜索：医院名称 + 系统名称（OA/HIS/电子病历等）
- 时间范围：近3-5年
- 采集频率：每周一次增量采集
```

**关键代码示例**：
```python
import scrapy
from selenium import webdriver
import pandas as pd
from datetime import datetime, timedelta

class GovProcurementSpider(scrapy.Spider):
    name = 'gov_procurement'
    
    # 医院列表（从你的Excel读取）
    hospitals = [
        '福州大学附属省立医院',
        '厦门大学附属第一医院',
        # ... 其他医院
    ]
    
    # 系统关键词
    system_keywords = [
        'OA', '办公自动化',
        'HIS', '医院信息系统',
        '电子病历', 'EMR',
        '人事系统',
        '科研教学',
        '财务系统',
        '物流系统',
        '护理系统'
    ]
    
    def start_requests(self):
        base_url = 'http://ccgp-fujian.gov.cn/notice/search'
        
        for hospital in self.hospitals:
            for keyword in self.system_keywords:
                # 构建搜索URL
                params = {
                    'keyword': f'{hospital} {keyword}',
                    'startDate': (datetime.now() - timedelta(days=365*3)).strftime('%Y-%m-%d'),
                    'endDate': datetime.now().strftime('%Y-%m-%d')
                }
                yield scrapy.Request(
                    url=f"{base_url}?{urlencode(params)}",
                    callback=self.parse,
                    meta={'hospital': hospital, 'system': keyword}
                )
```

#### B. 公众号文章爬虫

**难点**：微信公众号有反爬机制

**解决方案**：
1. **搜狗微信搜索** (weixin.sogou.com)
   - 可以搜索公众号历史文章
   - 相对容易爬取
   
2. **新榜/西瓜数据** (付费API)
   - 专业的公众号数据平台
   - 提供API接口

**免费替代方案**：
```python
# 通过搜狗微信搜索爬取
def search_wechat_articles(hospital_name, system_name):
    """
    搜索医院公众号发布的系统相关文章
    """
    url = f'https://weixin.sogou.com/weixin'
    params = {
        'type': 2,  # 2=文章搜索
        'query': f'{hospital_name} {system_name} 上线',
        'ie': 'utf8'
    }
    # 使用 selenium + 代理池
    # 提取文章标题、发布时间、内容摘要
```

#### C. 医院官网爬虫

**目标页面**：
- 新闻动态（系统上线新闻）
- 招标公告
- 信息公开栏目
- 科室介绍（了解业务流程）

**技术要点**：
```python
# 通用医院官网信息提取
class HospitalWebSpider:
    def __init__(self, hospital_name, website_url):
        self.hospital_name = hospital_name
        self.url = website_url
        
    def extract_news(self):
        """提取新闻动态中的系统相关信息"""
        keywords = ['系统上线', '信息化建设', '智慧医院', 
                   '数字化', '电子病历', 'HIS', 'OA']
        # 搜索包含关键词的新闻
        
    def extract_procurement(self):
        """提取招标采购信息"""
        # 定位招标公告栏目
        # 提取软硬件采购项目
```

### 2.2 AI辅助信息提取模块

使用 **大语言模型 (LLM)** 自动分析采购公告和新闻内容：

```python
from anthropic import Anthropic

def extract_system_info(text_content, hospital_name):
    """
    使用 Claude 提取系统信息
    """
    client = Anthropic(api_key="your-key")
    
    prompt = f"""
    请从以下政府采购公告或新闻内容中提取 {hospital_name} 的系统信息：
    
    需要提取的信息：
    1. 系统名称（OA/HIS/电子病历/人事/财务等）
    2. 供应商名称
    3. 软件产品名称和版本
    4. 硬件配置（服务器、存储、网络设备等）
    5. 项目金额
    6. 建设时间
    7. 技术架构（如：B/S架构、数据库类型等）
    
    内容：
    {text_content}
    
    请以JSON格式输出。
    """
    
    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=4096,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.content
```

### 2.3 数据匹配与整合模块

```python
import pandas as pd
import json

class DataIntegrator:
    def __init__(self, excel_path):
        self.excel_path = excel_path
        self.hospitals_data = {}
        
    def load_existing_data(self):
        """加载现有客户档案"""
        xl = pd.ExcelFile(self.excel_path)
        for sheet_name in xl.sheet_names:
            if sheet_name != '模板':
                df = pd.read_excel(xl, sheet_name=sheet_name, header=None)
                self.hospitals_data[sheet_name] = df
    
    def update_system_info(self, hospital_name, scraped_data):
        """
        更新医院的系统信息
        """
        # 找到对应的 sheet
        if hospital_name in self.hospitals_data:
            df = self.hospitals_data[hospital_name]
            
            # 定位到"软硬件建设情况"单元格
            # 自动填充系统信息
            row_idx = self._find_cell(df, '软硬件建设情况')
            
            # 格式化信息
            system_info = self._format_system_info(scraped_data)
            df.iloc[row_idx, col_idx] = system_info
            
            # 保存
            self.hospitals_data[hospital_name] = df
    
    def _format_system_info(self, data):
        """
        格式化系统信息
        例如：
        OA系统: 泛微OA V9.0 (供应商: XX公司, 建设时间: 2023年, 金额: 50万)
        HIS系统: 东软HIS (供应商: 东软集团, 建设时间: 2020年, 金额: 500万)
        """
        formatted = []
        for item in data:
            line = f"{item['system_name']}: {item['product']} "
            line += f"(供应商: {item['vendor']}, "
            line += f"建设时间: {item['date']}, "
            line += f"金额: {item['amount']})"
            formatted.append(line)
        return '\n'.join(formatted)
    
    def save_to_excel(self):
        """保存回Excel"""
        with pd.ExcelWriter(self.excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            for hospital_name, df in self.hospitals_data.items():
                df.to_excel(writer, sheet_name=hospital_name, index=False, header=False)
```

### 2.4 自动化运行流程

```python
# main.py - 主程序
import schedule
import time

def daily_update():
    """每日自动更新任务"""
    print(f"[{datetime.now()}] 开始数据采集...")
    
    # 1. 读取客户列表
    integrator = DataIntegrator('运维客户档案.xlsx')
    integrator.load_existing_data()
    hospitals = list(integrator.hospitals_data.keys())
    
    # 2. 爬取政府采购网
    gov_spider = GovProcurementSpider(hospitals)
    gov_data = gov_spider.run()
    
    # 3. 爬取医院官网
    for hospital in hospitals:
        hospital_url = get_hospital_url(hospital)  # 需要维护一个医院URL字典
        web_spider = HospitalWebSpider(hospital, hospital_url)
        web_data = web_spider.extract_all()
        
        # 4. AI提取信息
        extracted_info = []
        for item in (gov_data + web_data):
            info = extract_system_info(item['content'], hospital)
            extracted_info.append(info)
        
        # 5. 更新Excel
        integrator.update_system_info(hospital, extracted_info)
    
    # 6. 保存
    integrator.save_to_excel()
    print(f"[{datetime.now()}] 数据更新完成！")

# 设置每天凌晨2点运行
schedule.every().day.at("02:00").do(daily_update)

# 或者每周一运行
# schedule.every().monday.at("02:00").do(daily_update)

while True:
    schedule.run_pending()
    time.sleep(3600)  # 每小时检查一次
```

---

## 三、完整项目结构

```
hospital-info-collector/
├── config/
│   ├── hospitals.json          # 医院列表和URL配置
│   ├── system_keywords.json    # 系统关键词库
│   └── settings.py             # 全局配置
├── spiders/
│   ├── gov_procurement.py      # 政府采购网爬虫
│   ├── wechat_articles.py      # 公众号文章爬虫
│   └── hospital_website.py     # 医院官网爬虫
├── processors/
│   ├── ai_extractor.py         # AI信息提取
│   ├── data_cleaner.py         # 数据清洗
│   └── data_integrator.py      # 数据整合
├── utils/
│   ├── proxy_pool.py           # 代理池管理
│   ├── excel_handler.py        # Excel读写
│   └── logger.py               # 日志记录
├── data/
│   ├── raw/                    # 原始采集数据
│   ├── processed/              # 处理后数据
│   └── 运维客户档案.xlsx       # 最终输出
├── logs/                       # 日志文件
├── main.py                     # 主程序
├── requirements.txt            # 依赖包
└── README.md                   # 说明文档
```

---

## 四、关键依赖包

```txt
# requirements.txt
scrapy==2.11.0
selenium==4.15.0
pandas==2.1.0
openpyxl==3.1.2
anthropic==0.7.0
beautifulsoup4==4.12.2
requests==2.31.0
schedule==1.2.0
fake-useragent==1.4.0
redis==5.0.0  # 用于去重
pymongo==4.5.0  # 可选：存储到数据库
```

---

## 五、数据采集优先级建议

### 高优先级（必须实现）
1. ✅ **福建省政府采购网** - 最权威的数据源
2. ✅ **医院官网新闻** - 第一手信息
3. ✅ **医院招标公告** - 即将实施的项目

### 中优先级（建议实现）
4. ⭐ **中国政府采购网** - 补充数据
5. ⭐ **医院公众号** - 宣传性质的系统上线信息
6. ⭐ **供应商公众号** - 案例分享

### 低优先级（可选）
7. 🔸 **行业媒体报道** - 新闻稿
8. 🔸 **CHIMA案例库** - 优秀案例
9. 🔸 **展会新闻** - 行业动态

---

## 六、法律合规建议

⚠️ **重要提示**：
1. **遵守 robots.txt** - 检查网站是否允许爬取
2. **控制爬取频率** - 避免对服务器造成压力（建议 3-5秒/请求）
3. **使用代理IP池** - 避免被封
4. **数据仅供内部使用** - 不要公开传播
5. **公众号数据** - 建议使用官方API或付费平台

---

## 七、数据质量控制

### 7.1 数据验证规则
```python
def validate_system_info(data):
    """验证采集的数据质量"""
    required_fields = ['system_name', 'vendor', 'amount']
    
    # 必填字段检查
    for field in required_fields:
        if not data.get(field):
            return False, f"缺少字段: {field}"
    
    # 金额格式检查
    if not re.match(r'^\d+(\.\d+)?[万元]', data['amount']):
        return False, "金额格式错误"
    
    # 时间格式检查
    if not re.match(r'\d{4}年', data.get('date', '')):
        return False, "时间格式错误"
    
    return True, "验证通过"
```

### 7.2 人工审核机制
```python
def flag_for_review(data, confidence_score):
    """
    标记需要人工审核的数据
    置信度 < 0.7 的数据需要人工确认
    """
    if confidence_score < 0.7:
        # 将数据写入待审核队列
        review_queue.append({
            'hospital': data['hospital'],
            'content': data['raw_content'],
            'extracted': data['extracted_info'],
            'confidence': confidence_score,
            'timestamp': datetime.now()
        })
```

---

## 八、成本预估

### 免费方案（推荐起步）
- 爬虫服务器: 0元（本地运行或使用已有服务器）
- 代理IP: 50-100元/月（某些免费代理池）
- Claude API: 约100-300元/月（根据使用量）
- **总计**: 150-400元/月

### 进阶方案
- 云服务器: 100元/月
- 付费代理IP: 200-500元/月
- 公众号数据API (新榜): 500-1000元/月
- Claude API: 300-500元/月
- **总计**: 1100-2100元/月

---

## 九、实施时间表

### 第一阶段（1-2周）：基础框架搭建
- ✅ 搭建爬虫框架
- ✅ 实现政府采购网爬虫（最核心）
- ✅ 实现数据存储到Excel功能

### 第二阶段（2-3周）：功能完善
- ✅ 添加医院官网爬虫
- ✅ 集成AI信息提取
- ✅ 实现自动更新机制

### 第三阶段（1-2周）：优化与测试
- ✅ 数据质量验证
- ✅ 异常处理
- ✅ 性能优化

---

## 十、快速启动示例

```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 配置
cp config/settings.example.py config/settings.py
# 编辑 settings.py，填入 Claude API Key 等配置

# 3. 初始化数据库（可选）
python init_db.py

# 4. 运行单次采集测试
python main.py --mode test --hospital "福州大学附属省立医院"

# 5. 启动定时任务
python main.py --mode schedule
```

---

## 十一、监控与告警

```python
# 监控模块
import smtplib
from email.mime.text import MIMEText

class Monitor:
    def __init__(self):
        self.success_count = 0
        self.fail_count = 0
        
    def send_alert(self, message):
        """发送告警邮件"""
        msg = MIMEText(message)
        msg['Subject'] = '数据采集异常告警'
        msg['From'] = 'system@company.com'
        msg['To'] = 'admin@company.com'
        
        # 发送邮件
        
    def daily_report(self):
        """每日数据报告"""
        report = f"""
        数据采集日报 - {datetime.now().date()}
        
        成功采集: {self.success_count} 条
        失败: {self.fail_count} 条
        新增医院系统信息: {self.new_systems_count} 个
        更新医院: {len(self.updated_hospitals)} 家
        """
        self.send_alert(report)
```

---

## 十二、附加建议

### 12.1 建立系统知识库
建议建立一个 **医疗信息系统产品知识库**：
```json
{
  "OA系统": {
    "主流产品": ["泛微OA", "致远OA", "蓝凌OA", "华天动力OA"],
    "关键字": ["办公自动化", "协同办公", "流程管理"],
    "主要供应商": ["泛微网络", "致远互联", "蓝凌软件"]
  },
  "HIS系统": {
    "主流产品": ["东软HIS", "卫宁健康", "创业慧康", "嘉和美康"],
    "关键字": ["医院信息系统", "门诊系统", "住院系统"],
    "主要供应商": ["东软集团", "卫宁健康", "创业慧康"]
  }
}
```

### 12.2 竞品追踪
同步采集竞争对手的项目信息：
- 在政府采购网搜索竞争对手的中标项目
- 了解他们在哪些医院有业务
- 分析他们的产品和定价策略

### 12.3 数据可视化
建立数据看板：
```python
# 使用 Streamlit 或 Dash 构建可视化界面
import streamlit as st
import plotly.express as px

st.title("医院客户信息看板")

# 显示系统覆盖情况
fig = px.bar(df, x='hospital', y='system_count', title='各医院系统数量')
st.plotly_chart(fig)

# 显示供应商分布
fig2 = px.pie(df, names='vendor', title='供应商市场份额')
st.plotly_chart(fig2)
```

---

## 总结

这套方案的核心优势：
1. ✅ **自动化程度高** - 减少90%的人工搜集时间
2. ✅ **数据来源权威** - 政府采购网数据最可靠
3. ✅ **AI辅助提取** - 自动理解和结构化非结构化文本
4. ✅ **持续更新** - 定时任务保持数据新鲜度
5. ✅ **可扩展性强** - 易于添加新的数据源

**最快启动路径**：
1. 先实现政府采购网爬虫（2-3天）
2. 再添加医院官网爬虫（1-2天）
3. 最后集成AI提取（1天）

预计 **1-2周** 即可上线基础版本！
