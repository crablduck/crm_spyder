#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¦å»ºçœæ”¿åºœé‡‡è´­ç½‘çˆ¬è™«
Fujian Provincial Government Procurement Network Crawler

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨è¯†åˆ«å’Œå¤„ç†éªŒè¯ç 
2. æœç´¢æŒ‡å®šé‡‡è´­å•ä½ï¼ˆå¦‚"åŒ»é™¢"ï¼‰çš„å…¬å‘Šä¿¡æ¯
3. æå–æœç´¢ç»“æœåˆ—è¡¨æ•°æ®
4. çˆ¬å–è¯¦æƒ…é¡µé¢å†…å®¹
5. æ”¯æŒåˆ†é¡µå¤„ç†
6. æ•°æ®ä¿å­˜ä¸ºJSONå’ŒCSVæ ¼å¼
"""

import time
import json
import csv
import os
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
from tqdm import tqdm


class FujianProcurementCrawler:
    def __init__(self, headless=False, max_pages=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless (bool): æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            max_pages (int): æœ€å¤§çˆ¬å–é¡µæ•°ï¼ŒNoneè¡¨ç¤ºçˆ¬å–æ‰€æœ‰é¡µé¢
        """
        self.base_url = "https://zfcg.czt.fujian.gov.cn"
        self.search_url = "https://zfcg.czt.fujian.gov.cn/maincms-web/xmgg?titleType=xmgg"
        self.detail_url_pattern = "https://zfcg.czt.fujian.gov.cn/maincms-web/articleDetail"  # è¯¦æƒ…é¡µURLæ¨¡å¼
        self.max_pages = max_pages
        self.session = requests.Session()
        
        # ä¼šè¯çŠ¶æ€ç®¡ç†
        self.captcha_filled = False  # éªŒè¯ç å¡«å†™çŠ¶æ€
        self.session_active = False  # ä¼šè¯çŠ¶æ€
        
        # è®¾ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        

        
        # æ•°æ®å­˜å‚¨
        self.results = []
        self.detail_data = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = f"fujian_procurement_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    def check_captcha_status(self):
        """
        æ£€æŸ¥éªŒè¯ç çŠ¶æ€
        
        Returns:
            str: 'filled' - å·²å¡«å†™, 'empty' - æœªå¡«å†™, 'error' - æ£€æŸ¥å¤±è´¥
        """
        try:
            # æŸ¥æ‰¾éªŒè¯ç è¾“å…¥æ¡†
            captcha_input = self.driver.find_element(By.XPATH, "//input[@placeholder='è¯·è¾“å…¥éªŒè¯ç ']")
            captcha_value = captcha_input.get_attribute('value')
            
            if captcha_value and captcha_value.strip():
                print(f"âœ… æ£€æµ‹åˆ°å·²å¡«å†™çš„éªŒè¯ç : {captcha_value}")
                return 'filled'
            else:
                print("âš ï¸  éªŒè¯ç è¾“å…¥æ¡†ä¸ºç©º")
                return 'empty'
                
        except Exception as e:
            print(f"æ£€æŸ¥éªŒè¯ç çŠ¶æ€å¤±è´¥: {e}")
            return 'error'

    def wait_for_captcha_input(self, force_input=False):
        """
        ç­‰å¾…ç”¨æˆ·è¾“å…¥éªŒè¯ç 
        
        Args:
            force_input (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°è¾“å…¥éªŒè¯ç 
        """
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¡«å†™è¿‡éªŒè¯ç ä¸”ä¼šè¯ä»ç„¶æ´»è·ƒ
        if self.captcha_filled and self.session_active and not force_input:
            print("âœ… éªŒè¯ç å·²å¡«å†™ä¸”ä¼šè¯æ´»è·ƒï¼Œè·³è¿‡éªŒè¯ç è¾“å…¥")
            return
        
        print("\n" + "="*50)
        print("ğŸ” éªŒè¯ç è¾“å…¥")
        print("="*50)
        
        # é€šè¿‡ç»ˆç«¯è·å–ç”¨æˆ·è¾“å…¥çš„éªŒè¯ç 
        captcha_code = input("è¯·è¾“å…¥4ä½éªŒè¯ç : ").strip()
        
        if len(captcha_code) != 4 or not captcha_code.isdigit():
            print("âŒ éªŒè¯ç æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥4ä½æ•°å­—")
            return self.wait_for_captcha_input(force_input=True)
        
        try:
            # æŸ¥æ‰¾éªŒè¯ç è¾“å…¥æ¡†å¹¶è¾“å…¥éªŒè¯ç 
            captcha_input = self.wait.until(
                EC.presence_of_element_located((By.XPATH, "//input[contains(@placeholder, 'éªŒè¯ç ') or contains(@name, 'captcha') or contains(@id, 'captcha')]"))
            )
            
            # æ¸…ç©ºè¾“å…¥æ¡†å¹¶è¾“å…¥éªŒè¯ç 
            captcha_input.clear()
            captcha_input.send_keys(captcha_code)
            print(f"âœ… éªŒè¯ç  {captcha_code} å·²è¾“å…¥")
            
            # ç‚¹å‡»æŸ¥è¯¢æŒ‰é’® - ä½¿ç”¨æ›´çµæ´»çš„é€‰æ‹©å™¨
            print("ğŸ” æ­£åœ¨æŸ¥æ‰¾æŸ¥è¯¢æŒ‰é’®...")
            search_button = None
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨ï¼Œé’ˆå¯¹Element UIæ¡†æ¶çš„æŒ‰é’®ç»“æ„
            selectors = [
                "//button[contains(.//span, 'æŸ¥è¯¢')]",  # æŸ¥æ‰¾åŒ…å«spanä¸”spanæ–‡æœ¬åŒ…å«"æŸ¥è¯¢"çš„button
                "//button[.//span[text()='æŸ¥è¯¢']]",     # æŸ¥æ‰¾åŒ…å«spanä¸”spanæ–‡æœ¬ç­‰äº"æŸ¥è¯¢"çš„button
                "//button[contains(@class, 'el-button') and contains(.//span, 'æŸ¥è¯¢')]",  # Element UIæŒ‰é’®
                "//button[contains(text(), 'æŸ¥è¯¢')]",
                "//button[contains(normalize-space(text()), 'æŸ¥è¯¢')]",
                "//button[text()='æŸ¥è¯¢ ']",
                "//button[normalize-space(text())='æŸ¥è¯¢']",
                "//input[@type='button' and @value='æŸ¥è¯¢']",
                "//input[@type='submit' and contains(@value, 'æŸ¥è¯¢')]"
            ]
            
            for i, selector in enumerate(selectors):
                try:
                    print(f"ğŸ” å°è¯•é€‰æ‹©å™¨ {i+1}: {selector}")
                    search_button = self.driver.find_element(By.XPATH, selector)
                    print(f"âœ… æ‰¾åˆ°æŸ¥è¯¢æŒ‰é’®ï¼Œä½¿ç”¨é€‰æ‹©å™¨ {i+1}")
                    break
                except NoSuchElementException:
                    print(f"âŒ é€‰æ‹©å™¨ {i+1} æœªæ‰¾åˆ°æŒ‰é’®")
                    continue
            
            if search_button is None:
                print("âŒ æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°æŸ¥è¯¢æŒ‰é’®ï¼Œæ‰“å°é¡µé¢æºç è¿›è¡Œè°ƒè¯•...")
                # æ‰“å°æŒ‰é’®ç›¸å…³çš„HTML
                buttons = self.driver.find_elements(By.TAG_NAME, "button")
                print(f"é¡µé¢ä¸Šæ‰¾åˆ° {len(buttons)} ä¸ªæŒ‰é’®:")
                for i, btn in enumerate(buttons):
                    print(f"æŒ‰é’® {i+1}: text='{btn.text}', innerHTML='{btn.get_attribute('innerHTML')}'")
                raise Exception("æ— æ³•æ‰¾åˆ°æŸ¥è¯¢æŒ‰é’®")
            
            # ç‚¹å‡»æŒ‰é’®
            print("ğŸ–±ï¸ æ­£åœ¨ç‚¹å‡»æŸ¥è¯¢æŒ‰é’®...")
            search_button.click()
            print("âœ… å·²ç‚¹å‡»æŸ¥è¯¢æŒ‰é’®")
            
            # ç­‰å¾…ä¸€ä¸‹è®©é¡µé¢å“åº”
            time.sleep(2)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç é”™è¯¯æç¤º
            try:
                error_msg = self.driver.find_element(By.XPATH, "//*[contains(text(), 'éªŒè¯ç é”™è¯¯') or contains(text(), 'éªŒè¯ç å¤±æ•ˆ')]")
                if error_msg.is_displayed():
                    print("âŒ éªŒè¯ç é”™è¯¯æˆ–å¤±æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥")
                    return self.wait_for_captcha_input(force_input=True)
            except NoSuchElementException:
                pass  # æ²¡æœ‰é”™è¯¯æç¤ºï¼Œç»§ç»­
            
            # æ ‡è®°éªŒè¯ç å·²å¡«å†™å’Œä¼šè¯æ´»è·ƒ
            self.captcha_filled = True
            self.session_active = True
            
            print("âœ… éªŒè¯ç è¾“å…¥æˆåŠŸï¼Œç»§ç»­æ‰§è¡Œ...")
            return True
            
        except Exception as e:
            print(f"âŒ éªŒè¯ç è¾“å…¥å¤±è´¥: {e}")
            return self.wait_for_captcha_input(force_input=True)

    def search_procurement_unit(self, unit_name="åŒ»é™¢"):
        """
        æœç´¢æŒ‡å®šé‡‡è´­å•ä½
        
        Args:
            unit_name (str): é‡‡è´­å•ä½åç§°
            
        Returns:
            bool: æ˜¯å¦æœç´¢æˆåŠŸ
        """
        try:
            print(f"æœç´¢é‡‡è´­å•ä½: {unit_name}")
            
            # æ‰“å¼€æœç´¢é¡µé¢
            self.driver.get(self.search_url)
            time.sleep(3)
            
            # è¾“å…¥é‡‡è´­å•ä½
            unit_input = self.wait.until(
                EC.presence_of_element_located((By.XPATH, "//input[@placeholder='è¯·è¾“å…¥é‡‡è´­å•ä½']"))
            )
            unit_input.clear()
            unit_input.send_keys(unit_name)
            
            # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨è¾“å…¥éªŒè¯ç ï¼ˆæ™ºèƒ½æ£€æµ‹ï¼‰
            if not self.wait_for_captcha_input():
                print("ç”¨æˆ·å–æ¶ˆéªŒè¯ç è¾“å…¥")
                return False
            
            # ç‚¹å‡»æŸ¥è¯¢æŒ‰é’® - ä¿®å¤æŒ‰é’®å®šä½
            try:
                # å°è¯•å¤šç§å®šä½æ–¹å¼
                search_btn = None
                
                # æ–¹å¼1ï¼šé€šè¿‡æ–‡æœ¬å†…å®¹ï¼ˆåŒ…å«ç©ºæ ¼ï¼‰
                try:
                    search_btn = self.driver.find_element(By.XPATH, "//button[contains(text(), 'æŸ¥è¯¢')]")
                except:
                    pass
                
                # æ–¹å¼2ï¼šé€šè¿‡æ›´ç²¾ç¡®çš„æ–‡æœ¬åŒ¹é…
                if not search_btn:
                    try:
                        search_btn = self.driver.find_element(By.XPATH, "//button[normalize-space(text())='æŸ¥è¯¢']")
                    except:
                        pass
                
                # æ–¹å¼3ï¼šé€šè¿‡æŒ‰é’®ç±»å‹å’Œä½ç½®
                if not search_btn:
                    try:
                        buttons = self.driver.find_elements(By.TAG_NAME, "button")
                        for btn in buttons:
                            if "æŸ¥è¯¢" in btn.text:
                                search_btn = btn
                                break
                    except:
                        pass
                
                if search_btn:
                    search_btn.click()
                    print("æˆåŠŸç‚¹å‡»æŸ¥è¯¢æŒ‰é’®")
                else:
                    print("æœªæ‰¾åˆ°æŸ¥è¯¢æŒ‰é’®")
                    return False
                    
            except Exception as e:
                print(f"ç‚¹å‡»æŸ¥è¯¢æŒ‰é’®å¤±è´¥: {e}")
                return False
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            time.sleep(5)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
            try:
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æ˜¾ç¤º"è¯·å®Œæˆä¸Šæ–¹éªŒè¯ç æ“ä½œ"
                if "è¯·å®Œæˆä¸Šæ–¹éªŒè¯ç æ“ä½œ" in self.driver.page_source:
                    print("âŒ éªŒè¯ç éªŒè¯å¤±è´¥ï¼Œé¡µé¢ä»æ˜¾ç¤ºéªŒè¯ç æç¤º")
                    return False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœè¡¨æ ¼
                result_table = self.wait.until(
                    EC.presence_of_element_located((By.XPATH, "//table//tbody//tr"))
                )
                print("âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ°ç»“æœè¡¨æ ¼")
                return True
                
            except TimeoutException:
                print("âŒ æœªæ‰¾åˆ°æœç´¢ç»“æœè¡¨æ ¼æˆ–é¡µé¢åŠ è½½è¶…æ—¶")
                return False
                    
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return False

    def extract_search_results(self):
        """
        æå–å½“å‰é¡µé¢çš„æœç´¢ç»“æœ
        
        Returns:
            list: æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç é”™è¯¯æç¤º
            try:
                captcha_error = self.driver.find_element(By.XPATH, "//*[contains(text(), 'è¯·å®Œæˆä¸Šæ–¹éªŒè¯ç æ“ä½œ')]")
                if captcha_error.is_displayed():
                    print("âŒ æ£€æµ‹åˆ°éªŒè¯ç é”™è¯¯ï¼šè¯·å®Œæˆä¸Šæ–¹éªŒè¯ç æ“ä½œ")
                    print("è¯·åœ¨æµè§ˆå™¨ä¸­é‡æ–°è¾“å…¥éªŒè¯ç å¹¶ç‚¹å‡»æŸ¥è¯¢æŒ‰é’®")
                    return []
            except NoSuchElementException:
                pass  # æ²¡æœ‰éªŒè¯ç é”™è¯¯ï¼Œç»§ç»­æ‰§è¡Œ
            
            # ç­‰å¾…è¡¨æ ¼åŠ è½½
            try:
                table = self.wait.until(
                    EC.presence_of_element_located((By.XPATH, "//table//tbody"))
                )
            except TimeoutException:
                print("æœªæ‰¾åˆ°æœç´¢ç»“æœè¡¨æ ¼ï¼Œå¯èƒ½éœ€è¦é‡æ–°æœç´¢")
                return []
            
            # æå–è¡¨æ ¼è¡Œ
            rows = table.find_elements(By.TAG_NAME, "tr")
            
            if not rows:
                print("è¡¨æ ¼ä¸­æ²¡æœ‰æ•°æ®è¡Œ")
                return []
            
            # æ£€æŸ¥æ˜¯å¦åªæœ‰è¡¨å¤´æ²¡æœ‰æ•°æ®
            if len(rows) == 1:
                # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«è¡¨å¤´ä¿¡æ¯
                first_row_text = rows[0].text.strip()
                if any(header in first_row_text for header in ['åŒºåˆ’', 'é‡‡è´­æ–¹å¼', 'é‡‡è´­å•ä½', 'å…¬å‘Šæ ‡é¢˜', 'å‘å¸ƒæ—¶é—´']):
                    print("âš ï¸  è¡¨æ ¼åªæœ‰è¡¨å¤´ï¼Œæ²¡æœ‰å®é™…æ•°æ®ã€‚å¯èƒ½çš„åŸå› ï¼š")
                    print("   1. éªŒè¯ç æœªæ­£ç¡®è¾“å…¥")
                    print("   2. æœç´¢æ¡ä»¶æ²¡æœ‰åŒ¹é…çš„ç»“æœ")
                    print("   3. é¡µé¢åŠ è½½ä¸å®Œæ•´")
                    return []
            
            print(f"æ‰¾åˆ° {len(rows)} æ¡æœç´¢ç»“æœ")
            
            for i, row in enumerate(rows, 1):
                try:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 5:
                        # æå–åŸºæœ¬ä¿¡æ¯
                        district = cells[0].text.strip()
                        procurement_method = cells[1].text.strip()
                        procurement_unit = cells[2].text.strip()
                        
                        # æå–å…¬å‘Šæ ‡é¢˜å’Œé“¾æ¥
                        title_cell = cells[3]
                        
                        # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾é“¾æ¥
                        title_link = None
                        title = ""
                        detail_url = ""
                        
                        # æ–¹æ³•1: æŸ¥æ‰¾ <a> æ ‡ç­¾
                        try:
                            title_link = title_cell.find_element(By.TAG_NAME, "a")
                            title = title_link.text.strip()
                            detail_url = title_link.get_attribute('href')
                            print(f"âœ… ç¬¬{i}è¡Œæ‰¾åˆ°é“¾æ¥: {title[:30]}...")
                        except NoSuchElementException:
                            # æ–¹æ³•2: æŸ¥æ‰¾å¯ç‚¹å‡»çš„å…ƒç´ 
                            try:
                                clickable_elements = title_cell.find_elements(By.XPATH, ".//*[@onclick or @href or contains(@class, 'link') or contains(@class, 'clickable')]")
                                if clickable_elements:
                                    element = clickable_elements[0]
                                    title = element.text.strip()
                                    detail_url = element.get_attribute('href') or element.get_attribute('onclick')
                                    print(f"ğŸ”— ç¬¬{i}è¡Œæ‰¾åˆ°å¯ç‚¹å‡»å…ƒç´ : {title[:30]}...")
                                else:
                                    # æ–¹æ³•3: æ£€æŸ¥å•å…ƒæ ¼æ˜¯å¦æœ¬èº«å¯ç‚¹å‡»
                                    onclick = title_cell.get_attribute('onclick')
                                    if onclick:
                                        title = title_cell.text.strip()
                                        detail_url = self.extract_url_from_onclick(onclick)
                                        print(f"ğŸ“± ç¬¬{i}è¡Œå•å…ƒæ ¼å¯ç‚¹å‡»: {title[:30]}...")
                                    else:
                                        # æ–¹æ³•4: æŸ¥æ‰¾æ‰€æœ‰å­å…ƒç´ ï¼Œçœ‹æ˜¯å¦æœ‰éšè—çš„é“¾æ¥
                                        all_children = title_cell.find_elements(By.XPATH, ".//*")
                                        cell_text = title_cell.text.strip()
                                        
                                        print(f"ğŸ” ç¬¬{i}è¡Œè¯¦ç»†åˆ†æ:")
                                        print(f"   - å•å…ƒæ ¼æ–‡æœ¬: {cell_text[:50]}...")
                                        print(f"   - å­å…ƒç´ æ•°é‡: {len(all_children)}")
                                        print(f"   - å•å…ƒæ ¼HTML: {title_cell.get_attribute('outerHTML')[:200]}...")
                                        
                                        # æ£€æŸ¥æ˜¯å¦æœ‰dataå±æ€§åŒ…å«é“¾æ¥ä¿¡æ¯
                                        for attr in ['data-url', 'data-href', 'data-link', 'data-id']:
                                            attr_value = title_cell.get_attribute(attr)
                                            if attr_value:
                                                print(f"   - æ‰¾åˆ°å±æ€§ {attr}: {attr_value}")
                                                title = cell_text
                                                detail_url = attr_value
                                                break
                                        
                                        # æ–¹æ³•5: åŸºäºJavaScriptäº‹ä»¶æ„å»ºé“¾æ¥
                                        if not detail_url and cell_text:
                                            # å°è¯•ä»onclickäº‹ä»¶ä¸­æå–å‚æ•°
                                            onclick_attr = title_cell.get_attribute('onclick')
                                            if onclick_attr and 'articleDetail' in onclick_attr:
                                                detail_url = self.extract_url_from_onclick(onclick_attr)
                                                title = cell_text
                                                print(f"ğŸ”§ ç¬¬{i}è¡Œä»onclickæ„å»ºé“¾æ¥: {title[:30]}...")
                                            
                                            # æ–¹æ³•6: æ£€æŸ¥æ˜¯å¦æœ‰data-*å±æ€§å¯ä»¥æ„å»ºé“¾æ¥
                                            if not detail_url:
                                                data_attrs = self.extract_data_attributes(title_cell)
                                                if data_attrs:
                                                    detail_url = self.build_detail_url_from_data_attrs(data_attrs)
                                                    title = cell_text
                                                    print(f"ğŸ—ï¸ ç¬¬{i}è¡Œä»dataå±æ€§æ„å»ºé“¾æ¥: {title[:30]}...")
                                        
                                        if not detail_url and cell_text:
                                            print(f"âš ï¸  ç¬¬{i}è¡Œæ ‡é¢˜åˆ—æœ‰å†…å®¹ä½†æ— é“¾æ¥: {cell_text[:30]}... å°è¯•ç‚¹å‡»æ‰“å¼€è¯¦æƒ…")
                                            try:
                                                # æ»šåŠ¨åˆ°è¯¥å•å…ƒæ ¼å¹¶å°è¯•ç‚¹å‡»
                                                self.driver.execute_script("arguments[0].scrollIntoView({block:'center'});", title_cell)
                                                orig_url = self.driver.current_url
                                                orig_handles = self.driver.window_handles
                                                try:
                                                    title_cell.click()
                                                except Exception:
                                                    ActionChains(self.driver).move_to_element(title_cell).click().perform()
                                                
                                                # ç­‰å¾…æ–°çª—å£æˆ–URLå˜åŒ–
                                                new_handle = None
                                                try:
                                                    WebDriverWait(self.driver, 5).until(lambda d: len(d.window_handles) > len(orig_handles))
                                                    new_handle = [h for h in self.driver.window_handles if h not in orig_handles][0]
                                                    self.driver.switch_to.window(new_handle)
                                                except TimeoutException:
                                                    WebDriverWait(self.driver, 8).until(lambda d: d.current_url != orig_url)
                                                
                                                detail_url = self.driver.current_url
                                                title = cell_text
                                                print(f"ğŸ§­ ç¬¬{i}è¡Œç‚¹å‡»åè¿›å…¥è¯¦æƒ…: {detail_url}")
                                                
                                                # æå–è¯¦æƒ…æ•°æ®
                                                detail_data = self.extract_detail_page(detail_url)
                                                if detail_data:
                                                    self.detail_data.append(detail_data)
                                                
                                                # è¿”å›åˆ—è¡¨é¡µ
                                                if new_handle:
                                                    self.driver.close()
                                                    self.driver.switch_to.window(orig_handles[0])
                                                else:
                                                    self.driver.back()
                                                    self.wait.until(EC.presence_of_element_located((By.XPATH, "//table//tbody")))
                                                time.sleep(1)
                                                
                                                # æ„é€ ç»“æœé¡¹å¹¶åŠ å…¥åˆ—è¡¨
                                                publish_time = cells[4].text.strip()
                                                result = {
                                                    'district': district,
                                                    'procurement_method': procurement_method,
                                                    'procurement_unit': procurement_unit,
                                                    'title': title,
                                                    'detail_url': detail_url,
                                                    'publish_time': publish_time,
                                                    'crawl_time': datetime.now().isoformat()
                                                }
                                                results.append(result)
                                                print(f"âœ… é€šè¿‡ç‚¹å‡»æå–ç¬¬{i}è¡Œè¯¦æƒ…: {title[:50]}...")
                                                
                                            except Exception as click_err:
                                                print(f"âŒ ç¬¬{i}è¡Œç‚¹å‡»æ‰“å¼€è¯¦æƒ…å¤±è´¥: {click_err}")
                                                continue
                                        elif not cell_text:
                                            print(f"âŒ ç¬¬{i}è¡Œæ ‡é¢˜åˆ—ä¸ºç©º")
                                            continue
                            except Exception as e:
                                cell_text = title_cell.text.strip()
                                if cell_text:
                                    print(f"âš ï¸  ç¬¬{i}è¡Œæ ‡é¢˜åˆ—æœ‰å†…å®¹ä½†æ— é“¾æ¥: {cell_text[:30]}...")
                                else:
                                    print(f"âŒ ç¬¬{i}è¡Œæ ‡é¢˜åˆ—ä¸ºç©º")
                                continue
                        
                        # éªŒè¯æå–çš„æ•°æ®
                        if not title or not detail_url:
                            print(f"âŒ ç¬¬{i}è¡Œæ ‡é¢˜æˆ–é“¾æ¥ä¸ºç©ºï¼Œè·³è¿‡")
                            continue
                        
                        publish_time = cells[4].text.strip()
                        
                        result = {
                            'district': district,
                            'procurement_method': procurement_method,
                            'procurement_unit': procurement_unit,
                            'title': title,
                            'detail_url': detail_url,
                            'publish_time': publish_time,
                            'crawl_time': datetime.now().isoformat()
                        }
                        
                        results.append(result)
                        print(f"âœ… æˆåŠŸæå–ç¬¬{i}è¡Œ: {title[:50]}...")
                        
                    else:
                        print(f"âŒ ç¬¬{i}è¡Œåˆ—æ•°ä¸è¶³({len(cells)}åˆ—)ï¼Œè·³è¿‡")
                        
                except Exception as e:
                    print(f"âŒ å¤„ç†ç¬¬{i}è¡Œæ—¶å‡ºé”™: {e}")
                    continue
            
            if results:
                print(f"âœ… æˆåŠŸæå– {len(results)} æ¡æœç´¢ç»“æœ")
            else:
                print("âš ï¸  æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æœç´¢ç»“æœ")
                print("å»ºè®®æ£€æŸ¥ï¼š")
                print("   1. éªŒè¯ç æ˜¯å¦æ­£ç¡®è¾“å…¥")
                print("   2. æœç´¢æ¡ä»¶æ˜¯å¦åˆé€‚")
                print("   3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                    
        except Exception as e:
            print(f"âŒ æå–æœç´¢ç»“æœå¤±è´¥: {e}")
            
        return results
    
    def extract_url_from_onclick(self, onclick_attr):
        """
        ä»onclickå±æ€§ä¸­æå–URL
        
        Args:
            onclick_attr (str): onclickå±æ€§å€¼
            
        Returns:
            str: æ„å»ºçš„è¯¦æƒ…é¡µURL
        """
        try:
            if 'articleDetail' in onclick_attr:
                # è§£æonclickä¸­çš„å‚æ•°
                match = re.search(r"articleDetail\('([^']+)','([^']+)','([^']+)','([^']+)','([^']+)'\)", onclick_attr)
                if match:
                    type_param, id_param, plan_id, channel, source = match.groups()
                    return f"{self.detail_url_pattern}?type={type_param}&id={id_param}&planId={plan_id}&channel={channel}&soure={source}"
        except Exception as e:
            print(f"âŒ è§£æonclickå¤±è´¥: {e}")
        return None
    
    def extract_data_attributes(self, element):
        """
        æå–å…ƒç´ çš„dataå±æ€§
        
        Args:
            element: WebElement
            
        Returns:
            dict: dataå±æ€§å­—å…¸
        """
        data_attrs = {}
        try:
            for attr in ['data-id', 'data-type', 'data-plan-id', 'data-channel']:
                value = element.get_attribute(attr)
                if value:
                    data_attrs[attr.replace('data-', '')] = value
        except Exception as e:
            print(f"âŒ æå–dataå±æ€§å¤±è´¥: {e}")
        return data_attrs
    
    def build_detail_url_from_data_attrs(self, data_attrs):
        """
        ä»dataå±æ€§æ„å»ºè¯¦æƒ…é¡µURL
        
        Args:
            data_attrs (dict): dataå±æ€§å­—å…¸
            
        Returns:
            str: æ„å»ºçš„è¯¦æƒ…é¡µURL
        """
        try:
            if 'id' in data_attrs and 'type' in data_attrs:
                params = f"type={data_attrs['type']}&id={data_attrs['id']}"
                if 'plan-id' in data_attrs:
                    params += f"&planId={data_attrs['plan-id']}"
                if 'channel' in data_attrs:
                    params += f"&channel={data_attrs['channel']}"
                params += "&soure=ggxx"  # é»˜è®¤æ¥æº
                return f"{self.detail_url_pattern}?{params}"
        except Exception as e:
            print(f"âŒ æ„å»ºURLå¤±è´¥: {e}")
        return None

    def extract_detail_page(self, detail_url):
        """
        æå–è¯¦æƒ…é¡µé¢å†…å®¹
        
        Args:
            detail_url (str): è¯¦æƒ…é¡µé¢URL
            
        Returns:
            dict: è¯¦æƒ…é¡µé¢æ•°æ®
        """
        try:
            print(f"æ­£åœ¨æå–è¯¦æƒ…é¡µ: {detail_url}")
            
            # æ‰“å¼€è¯¦æƒ…é¡µ
            self.driver.get(detail_url)
            time.sleep(3)
            
            # è·å–é¡µé¢HTML
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æå–åŸºæœ¬ä¿¡æ¯
            detail_data = {
                'url': detail_url,
                'title': '',
                'publish_time': '',
                'content': '',
                'contract_info': {},
                'attachments': [],
                'crawl_time': datetime.now().isoformat()
            }
            
            # æå–æ ‡é¢˜
            title_elements = soup.find_all(['h1', 'h2', 'h3', 'h4'], string=re.compile(r'.*å…¬å‘Š.*'))
            if title_elements:
                detail_data['title'] = title_elements[0].get_text().strip()
            
            # æå–å‘å¸ƒæ—¶é—´
            time_pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})'
            time_match = re.search(time_pattern, soup.get_text())
            if time_match:
                detail_data['publish_time'] = time_match.group(1)
            
            # æå–æ­£æ–‡å†…å®¹
            content_div = soup.find('div', class_=re.compile(r'content|article|detail'))
            if content_div:
                detail_data['content'] = content_div.get_text().strip()
            else:
                detail_data['content'] = soup.get_text().strip()
            
            # æå–åˆåŒä¿¡æ¯ï¼ˆå¦‚æœæ˜¯åˆåŒå…¬å‘Šï¼‰
            if 'åˆåŒ' in detail_data.get('title', ''):
                contract_info = self.extract_contract_info(soup)
                detail_data['contract_info'] = contract_info
            
            # æå–é™„ä»¶é“¾æ¥
            attachments = []
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href and (href.endswith('.pdf') or href.endswith('.doc') or href.endswith('.docx')):
                    if not href.startswith('http'):
                        href = urljoin(self.base_url, href)
                    attachments.append({
                        'name': link.get_text().strip(),
                        'url': href
                    })
            detail_data['attachments'] = attachments
            
            return detail_data
            
        except Exception as e:
            print(f"æå–è¯¦æƒ…é¡µå¤±è´¥: {e}")
            return None

    def extract_contract_info(self, soup):
        """
        æå–åˆåŒä¿¡æ¯
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            dict: åˆåŒä¿¡æ¯
        """
        contract_info = {}
        
        try:
            text = soup.get_text()
            
            # æå–åˆåŒç¼–å·
            contract_no_match = re.search(r'åˆåŒç¼–å·[ï¼š:]\s*([^\n\r]+)', text)
            if contract_no_match:
                contract_info['contract_number'] = contract_no_match.group(1).strip()
            
            # æå–åˆåŒåç§°
            contract_name_match = re.search(r'åˆåŒåç§°[ï¼š:]\s*([^\n\r]+)', text)
            if contract_name_match:
                contract_info['contract_name'] = contract_name_match.group(1).strip()
            
            # æå–é¡¹ç›®ç¼–å·
            project_no_match = re.search(r'é¡¹ç›®ç¼–å·[ï¼š:]\s*([^\n\r]+)', text)
            if project_no_match:
                contract_info['project_number'] = project_no_match.group(1).strip()
            
            # æå–é‡‡è´­äººä¿¡æ¯
            buyer_match = re.search(r'é‡‡è´­äºº\(ç”²æ–¹\)[ï¼š:]\s*([^\n\r]+)', text)
            if buyer_match:
                contract_info['buyer'] = buyer_match.group(1).strip()
            
            # æå–ä¾›åº”å•†ä¿¡æ¯
            supplier_match = re.search(r'ä¾›åº”å•†\(ä¹™æ–¹\)[ï¼š:]\s*([^\n\r]+)', text)
            if supplier_match:
                contract_info['supplier'] = supplier_match.group(1).strip()
            
            # æå–åˆåŒé‡‘é¢
            amount_match = re.search(r'åˆåŒé‡‘é¢[ï¼š:]\s*([^\n\r]+)', text)
            if amount_match:
                contract_info['contract_amount'] = amount_match.group(1).strip()
            
            # æå–å±¥çº¦æœŸé™
            period_match = re.search(r'å±¥çº¦æœŸé™[ï¼š:]\s*([^\n\r]+)', text)
            if period_match:
                contract_info['performance_period'] = period_match.group(1).strip()
            
        except Exception as e:
            print(f"æå–åˆåŒä¿¡æ¯å¤±è´¥: {e}")
            
        return contract_info

    def get_total_pages(self):
        """
        è·å–æ€»é¡µæ•°
        
        Returns:
            int: æ€»é¡µæ•°
        """
        try:
            # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯
            page_info = self.driver.find_element(By.XPATH, "//span[contains(text(), 'é¡µ')]")
            page_text = page_info.text
            
            # æå–æ€»é¡µæ•°
            page_match = re.search(r'å…±\s*(\d+)\s*é¡µ', page_text)
            if page_match:
                return int(page_match.group(1))
            
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æœ€åä¸€é¡µçš„é¡µç 
            page_numbers = self.driver.find_elements(By.XPATH, "//ul[@class='el-pager']//li")
            if page_numbers:
                last_page = page_numbers[-1].text
                if last_page.isdigit():
                    return int(last_page)
                    
        except Exception as e:
            print(f"è·å–æ€»é¡µæ•°å¤±è´¥: {e}")
            
        return 1

    def go_to_next_page(self):
        """
        è·³è½¬åˆ°ä¸‹ä¸€é¡µ
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸè·³è½¬
        """
        try:
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
            next_btn = self.driver.find_element(By.XPATH, "//button[contains(@class, 'btn-next')]")
            
            if 'is-disabled' in next_btn.get_attribute('class'):
                print("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                return False
                
            next_btn.click()
            time.sleep(3)
            
            # ç­‰å¾…æ–°é¡µé¢åŠ è½½
            self.wait.until(
                EC.presence_of_element_located((By.XPATH, "//table//tbody//tr"))
            )
            
            return True
            
        except Exception as e:
            print(f"è·³è½¬ä¸‹ä¸€é¡µå¤±è´¥: {e}")
            return False

    def go_to_page(self, page_num):
        """
        è·³è½¬åˆ°æŒ‡å®šé¡µé¢
        
        Args:
            page_num (int): é¡µé¢å·ç 
            
        Returns:
            bool: æ˜¯å¦è·³è½¬æˆåŠŸ
        """
        try:
            print(f"è·³è½¬åˆ°ç¬¬ {page_num} é¡µ")
            
            # æŸ¥æ‰¾é¡µç è¾“å…¥æ¡†
            page_input = self.wait.until(
                EC.presence_of_element_located((By.XPATH, "//input[@placeholder='é¡µç ']"))
            )
            page_input.clear()
            page_input.send_keys(str(page_num))
            
            # ç‚¹å‡»å‰å¾€æŒ‰é’®
            go_btn = self.driver.find_element(By.XPATH, "//button[contains(text(), 'å‰å¾€')]")
            go_btn.click()
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            # éªŒè¯æ˜¯å¦è·³è½¬æˆåŠŸ
            try:
                self.wait.until(
                    EC.presence_of_element_located((By.XPATH, "//table//tbody//tr"))
                )
                print(f"æˆåŠŸè·³è½¬åˆ°ç¬¬ {page_num} é¡µ")
                return True
            except TimeoutException:
                print(f"è·³è½¬åˆ°ç¬¬ {page_num} é¡µå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"é¡µé¢è·³è½¬å¤±è´¥: {e}")
            return False

    def save_data(self):
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        """
        try:
            # ä¿å­˜æœç´¢ç»“æœä¸ºJSON
            results_file = os.path.join(self.output_dir, 'search_results.json')
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æœç´¢ç»“æœä¸ºCSV
            if self.results:
                csv_file = os.path.join(self.output_dir, 'search_results.csv')
                with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=self.results[0].keys())
                    writer.writeheader()
                    writer.writerows(self.results)
            
            # ä¿å­˜è¯¦æƒ…æ•°æ®ä¸ºJSON
            if self.detail_data:
                detail_file = os.path.join(self.output_dir, 'detail_data.json')
                with open(detail_file, 'w', encoding='utf-8') as f:
                    json.dump(self.detail_data, f, ensure_ascii=False, indent=2)
            
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.output_dir}")
            print(f"æœç´¢ç»“æœ: {len(self.results)} æ¡")
            print(f"è¯¦æƒ…æ•°æ®: {len(self.detail_data)} æ¡")
            
        except Exception as e:
            print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")

    def run(self, unit_name="åŒ»é™¢", extract_details=True):
        """
        è¿è¡Œçˆ¬è™«
        
        Args:
            unit_name (str): é‡‡è´­å•ä½åç§°
            extract_details (bool): æ˜¯å¦æå–è¯¦æƒ…é¡µé¢
        """
        try:
            print("å¼€å§‹è¿è¡Œç¦å»ºçœæ”¿åºœé‡‡è´­ç½‘çˆ¬è™«...")
            
            # æœç´¢é‡‡è´­å•ä½
            if not self.search_procurement_unit(unit_name):
                print("æœç´¢å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
                return
            
            # è·å–æ€»é¡µæ•°
            total_pages = self.get_total_pages()
            print(f"æ€»é¡µæ•°: {total_pages}")
            
            if self.max_pages:
                total_pages = min(total_pages, self.max_pages)
                print(f"é™åˆ¶çˆ¬å–é¡µæ•°: {total_pages}")
            
            # é€é¡µæå–æ•°æ®
            current_page = 1
            
            with tqdm(total=total_pages, desc="çˆ¬å–è¿›åº¦") as pbar:
                while current_page <= total_pages:
                    print(f"\næ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ...")
                    
                    # æå–å½“å‰é¡µæœç´¢ç»“æœ
                    page_results = self.extract_search_results()
                    self.results.extend(page_results)
                    
                    print(f"ç¬¬ {current_page} é¡µæå–åˆ° {len(page_results)} æ¡è®°å½•")
                    
                    # æå–è¯¦æƒ…é¡µé¢ï¼ˆå¯é€‰ï¼‰
                    if extract_details:
                        for result in page_results:
                            detail_data = self.extract_detail_page(result['detail_url'])
                            if detail_data:
                                self.detail_data.append(detail_data)
                            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                    # è·³è½¬åˆ°ä¸‹ä¸€é¡µ
                    if current_page < total_pages:
                        if not self.go_to_next_page():
                            break
                    
                    current_page += 1
                    pbar.update(1)
                    
                    # å®šæœŸä¿å­˜æ•°æ®
                    if current_page % 10 == 0:
                        self.save_data()
            
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self.save_data()
            
            print("\nçˆ¬å–å®Œæˆï¼")
            
        except Exception as e:
            print(f"è¿è¡Œå‡ºé”™: {e}")
        finally:
            self.close()

    def close(self):
        """
        å…³é—­æµè§ˆå™¨
        """
        try:
            self.driver.quit()
        except:
            pass


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ç¦å»ºçœæ”¿åºœé‡‡è´­ç½‘çˆ¬è™«")
    print("=" * 50)
    
    # é…ç½®å‚æ•°
    unit_name = input("è¯·è¾“å…¥é‡‡è´­å•ä½åç§° (é»˜è®¤: åŒ»é™¢): ").strip() or "åŒ»é™¢"
    
    max_pages_input = input("è¯·è¾“å…¥æœ€å¤§çˆ¬å–é¡µæ•° (é»˜è®¤: å…¨éƒ¨): ").strip()
    max_pages = int(max_pages_input) if max_pages_input.isdigit() else None
    
    extract_details = input("æ˜¯å¦æå–è¯¦æƒ…é¡µé¢? (y/n, é»˜è®¤: y): ").strip().lower() != 'n'
    
    headless = input("æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼? (y/n, é»˜è®¤: n): ").strip().lower() == 'y'
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = FujianProcurementCrawler(headless=headless, max_pages=max_pages)
    
    try:
        # è¿è¡Œçˆ¬è™«
        crawler.run(unit_name=unit_name, extract_details=extract_details)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"ç¨‹åºå¼‚å¸¸: {e}")
    finally:
        crawler.close()


if __name__ == "__main__":
    main()